{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyOdfTJr3jS8igppktAvkYoq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmrn-crmns-phl/abm/blob/master/hdl_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go2RFbvcduLC",
        "outputId": "cb476fa4-a89d-4bdc-b594-4752e1425a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapy in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from mediapy) (7.34.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapy) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapy) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mediapy) (10.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (71.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->mediapy) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->mediapy) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapy) (1.16.0)\n",
            "Requirement already satisfied: mujoco in /usr/local/lib/python3.10/dist-packages (3.2.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.9.4)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco) (2.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.26.4)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (6.4.5)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (4.12.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (3.20.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement xvfb (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for xvfb\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install mediapy\n",
        "!pip install mujoco\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "\n",
        "!pip install xvfb -y\n",
        "\n",
        "import time\n",
        "import mediapy as media\n",
        "import mujoco\n",
        "import mujoco.viewer\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import xml.etree.ElementTree as ET\n",
        "import sys\n",
        "\n",
        "tree = ET.parse(\"/content/xml/trexml.xml\")\n",
        "root = tree.getroot()\n",
        "\n",
        "cam = mujoco.MjvCamera()\n",
        "# opt = mujoco.MjvOption()\n",
        "\n",
        "\"\"\" initialize state size and action size, action size is the number of actuators\"\"\"\n",
        "\n",
        "\"\"\" Q NETWORK \"\"\"\n",
        "\n",
        "\n",
        "class MultiJointQNetwork(nn.Module):\n",
        "    def __init__(self, input_size=108, output_size=54):\n",
        "        super(MultiJointQNetwork, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # print(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        # x = torch.tanh(self.fc4(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "def update_q_network(\n",
        "    state, action, reward, next_state, done, q_network, optimizer, loss_function\n",
        "):\n",
        "\n",
        "    state_normalized = (state - state.mean()) / (state.std() + 1e-8)\n",
        "    next_state_normalized = (next_state - next_state.mean()) / (next_state.std() + 1e-8)\n",
        "\n",
        "    next_q_values = q_network(next_state_normalized).detach()\n",
        "    next_action_value = next_q_values.max()\n",
        "\n",
        "    if not isinstance(reward, torch.Tensor):\n",
        "        reward = torch.tensor(reward, dtype=torch.float32).view(1, -1)\n",
        "\n",
        "    control_signal = [0.0] * len(get_actuators())\n",
        "\n",
        "    # # Compute the target Q-value\n",
        "    # next_q_values = q_network(next_state).detach()\n",
        "    # next_action_value = next_q_values.max()\n",
        "\n",
        "    # if not isinstance(next_action_value, torch.Tensor):\n",
        "    #     next_action_value = torch.tensor(next_action_value, dtype=torch.float32)\n",
        "\n",
        "    target = reward + (\n",
        "        0.99 * next_action_value * (1 - done)\n",
        "    )  # Discount factor gamma = 0.99\n",
        "\n",
        "    target = target.view(1, -1).float()\n",
        "\n",
        "    current_q_values = q_network(state_normalized)\n",
        "\n",
        "    action = action.view(-1, 1).long()\n",
        "\n",
        "    action = action.view(1, 54)\n",
        "\n",
        "    # current_q_value = q_network(state)[0]\n",
        "    current_q_value = current_q_values\n",
        "\n",
        "    # Compute loss (difference between predicted Q-value and target Q-value)\n",
        "    loss = loss_function(current_q_value, target)\n",
        "\n",
        "    # Perform backpropagation to update Q-network\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def select_action(state, q_network):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # print(\"controls\")\n",
        "        control_signals = q_network(state) *200\n",
        "\n",
        "        # print(control_signals)\n",
        "\n",
        "    # exploration_noise = np.random.normal(0, 0.1, size=control_signals.shape)\n",
        "    # action_with_noise = control_signals + torch.tensor(exploration_noise, dtype=torch.float32)\n",
        "\n",
        "    return control_signals  # Return the control signal as the chosen action\n",
        "\n",
        "\n",
        "\n",
        "def check_termination(data, model, state, max_time=15.0):\n",
        "\n",
        "    # print(f\"Gravity: {model.opt.gravity}\")\n",
        "    # print(f\"Gravity :  {data.qpos}\")\n",
        "    # print(f\"initial joint positions {data.qvel}\")\n",
        "\n",
        "    # print(f\"control signals : {data.ctrl}\")\n",
        "    # print(f\"solver timestep: {model.opt.timestep}\")\n",
        "    # print(f\"Integrator: {model.opt.integrator}\")\n",
        "\n",
        "    # time.sleep(1)\n",
        "    \"\"\"\n",
        "    Checks whether the current episode should terminate.\n",
        "\n",
        "    Parameters:\n",
        "    - data: The MuJoCo data object.\n",
        "    - max_time: Maximum time (in seconds) for the episode.\n",
        "\n",
        "    Returns:\n",
        "    - done: Boolean indicating whether the episode should terminate.\n",
        "    \"\"\"\n",
        "    # print(data.geom_xpos[17])\n",
        "    # mujoco.mj_kinematics(model, data)\n",
        "\n",
        "    \"\"\" this gets the height of the pelvis -- z direction up and down.  in meters?\n",
        "    ok i need to print the hip height and nothing else, I need to know when\n",
        "    gravity hits it , when does it begin to fall down\n",
        "    the videos sometimes fall from gravity AFTER it tests a bunch of random limb movements\n",
        "    then the limbs go limp, they don't move at all once gravity hits and the bot falls down \"\"\"\n",
        "    height = data.geom(\"HIPS\").xpos[2]\n",
        "\n",
        "    # print(height)\n",
        "\n",
        "    if height < 1.0:\n",
        "        print(\"Terminated : T. rex fell down!!!\")\n",
        "        print(3 - height)\n",
        "        print(height)\n",
        "\n",
        "        print(\"------------------\\n\\n\\n\")\n",
        "        return True\n",
        "\n",
        "    # Condition 2: Check if the maximum time has been reached\n",
        "    if data.time >= max_time:\n",
        "        print(data.time)\n",
        "        print(max_time)\n",
        "        print(\"Terminated: Maximum time reached.\")\n",
        "        return True\n",
        "\n",
        "    # Additional conditions could include joint angle limits, distance covered, etc.\n",
        "\n",
        "    # If no termination conditions are met\n",
        "    return False\n",
        "\n",
        "\n",
        "def compute_reward(data, previous_hip_pos):\n",
        "    \"\"\"Note ::: id 1 is torso3_1_1, id 2 is torso_3_1_2, skull is 17\n",
        "    ok if the forward reward is the movement forward of the hips, it can be the difference between\n",
        "    the previous x position of the hips, and the current x position\n",
        "    since my model points in the -x direction, we want the torso position to reward increasingly negative positions\n",
        "    \"\"\"\n",
        "\n",
        "    hips_x_coord = data.geom(\"HIPS\").xpos[0]\n",
        "\n",
        "    hip_height = data.geom(\"HIPS\").xpos[2]\n",
        "\n",
        "    if previous_hip_pos > hips_x_coord:\n",
        "\n",
        "        forward_motion_reward = 1 + previous_hip_pos - hips_x_coord\n",
        "\n",
        "    else:\n",
        "\n",
        "        forward_motion_reward = 0\n",
        "\n",
        "    # print(\"forward motion reward\")\n",
        "\n",
        "    # print(forward_motion_reward)\n",
        "\n",
        "    stability_penalty = -1 if hip_height < 2.0 else 0\n",
        "\n",
        "    reward = forward_motion_reward + stability_penalty\n",
        "\n",
        "    return reward\n",
        "\n",
        "    # current_x_pos = data.geom(\"HIPS\").xpos[0]\n",
        "    # print(current_x_pos)\n",
        "\n",
        "    # forward_motion =  abs(previous_x_pos - current_x_pos)\n",
        "    # print(forward_motion)\n",
        "    # mj_forward(model, data) could be real\n",
        "\n",
        "    # forward_motion = current_x_pos - previous_hip_position\n",
        "\n",
        "def get_actuators():\n",
        "\n",
        "    ids = {}\n",
        "    num = 0\n",
        "    for motor in root.findall(\".//motor\"):\n",
        "        motor_name = motor.get(\"name\")\n",
        "        if motor_name is not None:\n",
        "            # print(motor_name)\n",
        "            num = num + 1\n",
        "            ids.update({motor_name: data.actuator(motor_name).id})\n",
        "\n",
        "    if len(ids) < 1:\n",
        "        for motor in root.findall(\".//general\"):\n",
        "            motor_name = motor.get(\"name\")\n",
        "            if motor_name is not None:\n",
        "                # print(motor_name)\n",
        "                num = num + 1\n",
        "                ids.update({motor_name: data.actuator(motor_name).id})\n",
        "        # ids.append(data.joint(joint_name).id)\n",
        "    return ids\n",
        "\n",
        "def get_joint_positions():\n",
        "    ids = []\n",
        "    for joint in root.findall(\".//joint\"):\n",
        "        joint_name = joint.get(\"name\")\n",
        "        if joint_name is not None:\n",
        "            # data.geom(\"HIPS\").xpos[2]\n",
        "\n",
        "            # print(joint_name)\n",
        "\n",
        "            # print(data.joint(joint_name).id)\n",
        "\n",
        "            ids.append(\n",
        "                (joint_name, data.joint(joint_name).id, data.joint(joint_name).qpos)\n",
        "            )\n",
        "            # ids.append(data.joint(joint_name).id)\n",
        "    # print(ids)\n",
        "    return ids\n",
        "\n",
        "def get_joint_names_positions():\n",
        "\n",
        "    names = []\n",
        "\n",
        "    for joint in root.findall(\".//joint\"):\n",
        "\n",
        "        joint_name = joint.get(\"name\")\n",
        "\n",
        "        if joint_name is not None:\n",
        "\n",
        "            b = (model.joint(joint_name).id, joint_name, model.joint(joint_name).pos)\n",
        "\n",
        "            \"\"\"model joint pos == the x y and z coordinates\n",
        "                model joint range == the -1 to 1 range of joint motion ; \"\"\"\n",
        "\n",
        "            names.append(b)\n",
        "\n",
        "    return names\n",
        "\n",
        "\n",
        "def get_state(data):\n",
        "    joint_positions = data.qpos[:54]\n",
        "    joint_velocities = data.qvel[:54]\n",
        "\n",
        "    # joint_positions_fn = get_joint_positions()\n",
        "\n",
        "    # state = torch.tensor(state, joint_positions + joint_velocities, dtype=torch.float32)\n",
        "\n",
        "    state = np.concatenate([joint_positions, joint_velocities])\n",
        "\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    # print(\"state of the union\")\n",
        "    # print(state.shape)\n",
        "\n",
        "    # print(data.joint(\"torso1_joint2\").qpos)\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "model = mujoco.MjModel.from_xml_path(\n",
        "    \"/content/xml/trexml.xml\"\n",
        ")\n",
        "data = mujoco.MjData(model)\n",
        "\n",
        "input_size = 108  # Assuming 54 joints with position and velocity (54x2)\n",
        "output_size = 54  # One control signal per joint\n",
        "\n",
        "q_network = MultiJointQNetwork(input_size, output_size)\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=0.003)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "state_size = len(np.concatenate([data.qpos, data.qvel]))\n",
        "action_size = model.nu\n",
        "\n",
        "results = []\n",
        "# all_results = []\n",
        "\n",
        "scene_option = mujoco.MjvOption()\n",
        "\"\"\" display joint axes and show contact points\"\"\"\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_JOINT] = True\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTFORCE] = True\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_ACTUATOR] = True\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_ACTIVATION] = True\n",
        "# scene_option.flags[mujoco.mjtTextureRole.mjTEXROLE_OPACITY] = False\n",
        "\n",
        "# print(scene_option.flags[mujoco.mjtTextureRole.mjTEXROLE_OPACITY])\n",
        "# sys.exit()\n",
        "\n",
        "num_actions = model.nu\n",
        "weights = np.random.randn(num_actions)\n",
        "\n",
        "viewer = mujoco.viewer.launch_passive(model, data)\n",
        "print(\"------\")\n",
        "\n",
        "eps = range(1000)\n",
        "sim_duration = 10\n",
        "perturbation = 1e-7\n",
        "\n",
        "n_steps = int(sim_duration / model.opt.timestep)\n",
        "sim_time = np.zeros(n_steps)\n",
        "angle = np.zeros(n_steps)\n",
        "energy = np.zeros(n_steps)\n",
        "\n",
        "\n",
        "best_model_path = \"/content/trained/best_model.pth\"\n",
        "latest_model_path = best_model_path = (\n",
        "    \"/content/trained/latest_model.pth\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "best_reward = -float(\"inf\")\n",
        "\n",
        "for k in eps:\n",
        "\n",
        "    print(str(k) + \" of \" + str(len(eps)) + \" episodes.\")\n",
        "\n",
        "    mujoco.mj_resetData(model, data)\n",
        "    # mujoco.mj_\n",
        "    # print(state)\n",
        "\n",
        "    \"\"\" i am pretty sure qvel = joint velocities , each\n",
        "        joint should have a velocity set either by index number\n",
        "        or actuator I think? \"\"\"\n",
        "\n",
        "    state = get_state(data)\n",
        "    model.opt.gravity[:] = [0, 0, -9.81]\n",
        "    data.qvel[:] += perturbation * model.nv\n",
        "    # print(state)\n",
        "    # sys.exit()\n",
        "\n",
        "    \"\"\" reset the whole thing, start a new episode from zero at the beginning of each loop\n",
        "    I don't understand how the reward is saved or used with a policy, I bet that's what I need to figure out\"\"\"\n",
        "\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    frames = []\n",
        "\n",
        "    mujoco.mj_resetDataKeyframe(model, data, 0)\n",
        "\n",
        "    \"\"\" resetting the data keyframe to 0 ?\"\"\"\n",
        "\n",
        "    # data.qfrc_actuator\n",
        "\n",
        "    with mujoco.Renderer(model, height=480, width=640) as renderer:\n",
        "\n",
        "        renderer.update_scene(data)\n",
        "\n",
        "        mujoco.mj_forward(model, data)\n",
        "\n",
        "        # with mujoco.viewer.launch_passive(model, data) as viewer:\n",
        "        t = 0\n",
        "\n",
        "        n_steps = 0\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            tic = time.time()\n",
        "\n",
        "            mujoco.mj_step(model, data)\n",
        "            # mujoco.mj_kinematics(model, data)\n",
        "\n",
        "            \"\"\"i think previous_hip_position needs to be a global?\"\"\"\n",
        "            previous_hip_position = data.geom(\"HIPS\").xpos[0]\n",
        "\n",
        "            action = select_action(state, q_network)  # Select action\n",
        "\n",
        "            # for joint_idx in range(54):\n",
        "\n",
        "            #     if joint_idx!=0:\n",
        "\n",
        "\n",
        "\n",
        "                #     print(\"joint_idx\")\n",
        "                #     print(joint_idx)\n",
        "                #     print(data.ctrl[joint_idx])\n",
        "\n",
        "                    # \"\"\" I think data.ctrl is set by action, but why won't this thing work on indices\n",
        "                    #     \"\"\"\n",
        "\n",
        "\n",
        "            data.ctrl = action\n",
        "\n",
        "\n",
        "            # mujoco.mj_step(model, data)\n",
        "\n",
        "            reward = compute_reward(data, previous_hip_position)\n",
        "\n",
        "            if random.randrange(150) == 86:\n",
        "        #         print(data.qfrc_actuator[joint_idx])\n",
        "                print(\"\\ntime:\")\n",
        "                print(time.strftime(\"%H:%M.%S\"))\n",
        "                print(\"[episode]\")\n",
        "                print(k)\n",
        "                print(\"[reward]\")\n",
        "                print(reward)\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            renderer.update_scene(data, scene_option=scene_option)\n",
        "\n",
        "            if total_reward > best_reward:\n",
        "\n",
        "                best_reward = total_reward\n",
        "\n",
        "                torch.save(q_network.state_dict(), best_model_path)\n",
        "\n",
        "                print(f\"new best model saved with reward: {best_reward}\")\n",
        "\n",
        "            pixels = renderer.render()\n",
        "\n",
        "            frames.append(pixels)\n",
        "\n",
        "            next_state = get_state(data)\n",
        "\n",
        "            hips_x_coord = data.geom(\"HIPS\").xpos[0]\n",
        "\n",
        "            # print(hips_x_coord)\n",
        "            # print(f\"Hips x coord: {hips_x_coord}\")\n",
        "            hips_x_coord = data.geom(\"HIPS\").xpos[2]\n",
        "\n",
        "            done = check_termination(data, model, state)\n",
        "\n",
        "            state_normalized = (state - state.mean()) / (state.std() + 1e-8)\n",
        "\n",
        "            update_q_network(\n",
        "                state,\n",
        "                action,\n",
        "                reward,\n",
        "                next_state,\n",
        "                done,\n",
        "                q_network,\n",
        "                optimizer,\n",
        "                loss_function,\n",
        "            )\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\" next state is new and different than the first state, the loop updates itself i think\"\"\"\n",
        "\n",
        "\n",
        "    print(f\"Total Reward: {total_reward}\")\n",
        "    results.append(total_reward)\n",
        "\n",
        "    media.write_video(\n",
        "        \"/content/results_videos/loop_\"\n",
        "        + str(k)\n",
        "        + \".mp4\",\n",
        "        frames,\n",
        "        fps=60\n",
        "    )\n",
        "\n",
        "    # if k % 100==0:\n",
        "\n",
        "    #     media.write_video(\n",
        "    #         \"/Users/cameronpahl/projects/HIPS_DONT_LIE/LearningHumanoidWalking/results_videos/loop_\"\n",
        "    #         + str(k)\n",
        "    #         + \".mp4\",\n",
        "    #         frames,\n",
        "    #         fps=60\n",
        "    #     )\n",
        "\n",
        "    # /content/feedback\n",
        "\n",
        "    with open(\"/content/feedback/feedback.txt\",\"a\") as f:\n",
        "\n",
        "        print(f\"Episode {str(k) + str(1)}: Total Reward = {total_reward:.2f}\", file=f)\n",
        "        print(f\"Total reward FINAL: {total_reward}\", file=f)\n",
        "        print(f\"ALL REWARDS FINAL: {results}\", file=f)\n",
        "        print(\"HIGHEST REWARD\", file=f)\n",
        "        print(max(enumerate(results), key=lambda x: x[1]), file=f)\n",
        "\n",
        "        print(f\"Episode {str(k) + str(1)}: Total Reward = {total_reward:.2f}\", file=f)\n",
        "        print(\"Total number of DoFs in the model:\", model.nv, file=f)\n",
        "        # print(\"Generalized positions:\", data.qpos)\n",
        "        # print(\"Generalized velocities:\", data.qvel)\n",
        "\n",
        "        # media.show_video(frames, fps=30)\n",
        "        print(f\"Joint Positions: {data.qpos}\", file=f)\n",
        "        print(f\"Joint Velocities: {data.qvel}\", file=f)\n",
        "        print(f\"Torques: {data.qfrc_actuator}\", file=f)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}